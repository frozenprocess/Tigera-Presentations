# Cluster creation

First, let's use `eksctl` to create a single EKS control plane and one `amd64` node.
```
eksctl create cluster --config-file - <<EOF
apiVersion: eksctl.io/v1alpha5
kind: ClusterConfig
metadata:
  name: calico-ebpf-aws
  region: us-west-2
  version: '1.22'
nodeGroups:
  - name: ng-amd64
    instanceType: m5.large
    minSize: 1
    maxSize: 2
    desiredCapacity: 1
    amiFamily: Bottlerocket
    ami: auto-ssm
    iam:
      attachPolicyARNs:
      - arn:aws:iam::aws:policy/AmazonEKSWorkerNodePolicy
      - arn:aws:iam::aws:policy/AmazonEKS_CNI_Policy
      - arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryReadOnly
      - arn:aws:iam::aws:policy/AmazonSSMManagedInstanceCore
EOF
```

Use the following command to update `coredns`, `aws-node` manifests.
```
eksctl utils update-coredns --cluster calico-ebpf-aws --approve
eksctl utils update-aws-node --cluster calico-ebpf-aws --approve
eksctl utils update-kube-proxy --cluster calico-ebpf-aws --approve
```

Add an `arm64` node to the mix.
```
eksctl create nodegroup  --cluster calico-ebpf-aws --name ng-arm64   --nodes 1  --nodes-min 1  --nodes-max 2  --node-type m6g.large
```

Use the following command to temporarily store node names in your current terminal session.
```
FIRST_NODE=`kubectl get node | awk 'FNR==2{print $1}'`
SECOND_NODE=`kubectl get node | awk 'FNR==3{print $1}'`
```

Use the following command to download the benchmarking utility and navigate into its folder.
```
git clone --branch multi-stream git@github.com:frozenprocess/k8s-bench-suite.git
cd k8s-bench-suite
```

> **Note:** Change the `-P` and `-m` value to match your instance `VCPU capabilities`.
Use the following command to run a benchmark.
```
./knb -v -P 4 -m 4 -cn $FIRST_NODE -sn $SECOND_NODE -o data -f aws-cni.knbdata --plot -pd aws-cni/
```

Install the Tigera Calico operator.
```
kubectl create -f https://projectcalico.docs.tigera.io/archive/v3.22/manifests/tigera-operator.yaml
```

Create an `Installation` resource.
```
kubectl apply -f -<<EOF
apiVersion: operator.tigera.io/v1
kind: Installation
metadata:
  name: default
spec:
  cni:
    type: AmazonVPC
  flexVolumePath: /var/lib/kubelet/plugins
  # Enables provider-specific settings required for compatibility.
  kubernetesProvider: EKS
EOF
```

Use the following command to determine the FQDN of your EKS control-plane.
```
kubectl get cm -n kube-system kube-proxy -o yaml | grep server
```

Use the following command to directly connect Calico to the Kubernetes API server.
```
kubectl apply -f - <<EOF
kind: ConfigMap
apiVersion: v1
metadata:
  name: kubernetes-services-endpoint
  namespace: tigera-operator
data:
  KUBERNETES_SERVICE_HOST: "<<CHANGE_THIS>>"
  KUBERNETES_SERVICE_PORT: "443"
EOF
```

Use the following command to enable Calico's `eBPF` dataplane.
```
kubectl patch installation.operator.tigera.io default --type merge -p '{"spec":{"calicoNetwork":{"linuxDataplane":"BPF", "hostPorts":null}}}'
```

Use the following command to disable `kube-proxy`.
```
kubectl patch ds -n kube-system kube-proxy -p '{"spec":{"template":{"spec":{"nodeSelector":{"non-calico": "true"}}}}}'
```

> **Note:** Change the `-P` and `-m` value to match your instance `VCPU capabilities`.
Run the benchmark again.
```
./knb -v -P 4 -m 4 -cn $FIRST_NODE -sn $SECOND_NODE -o data -f aws-cni_calico_ebpf.knbdata --plot -pd aws-cni_calico_ebpf/
```
# Clean-up
Use the following command to delete the resources that you created in AWS.
```
eksctl delete cluster calico-ebpf-aws
```
