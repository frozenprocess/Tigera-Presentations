Navigate into the `01.manifest-install` folder and issue the following command to create a multi-node cluster.
```
kind create cluster --config - <<EOF
kind: Cluster
apiVersion: kind.x-k8s.io/v1alpha4
networking:
  # the default CNI will not be installed
  disableDefaultCNI: true
nodes:
- role: control-plane
  image: kindest/node:v1.23.6@sha256:b1fa224cc6c7ff32455e0b1fd9cbfd3d3bc87ecaa8fcb06961ed1afb3db0f9ae
  extraMounts:
  - hostPath: cni-installer.sh
    containerPath: /root/cni-installer.sh
- role: worker
  image: kindest/node:v1.23.6@sha256:b1fa224cc6c7ff32455e0b1fd9cbfd3d3bc87ecaa8fcb06961ed1afb3db0f9ae
  extraMounts:
  - hostPath: cni-installer.sh
    containerPath: /root/cni-installer.sh
- role: worker
  image: kindest/node:v1.23.6@sha256:b1fa224cc6c7ff32455e0b1fd9cbfd3d3bc87ecaa8fcb06961ed1afb3db0f9ae
  extraMounts:
  - hostPath: cni-installer.sh
    containerPath: /root/cni-installer.sh
EOF
```
Use the following command to determine the Kubernetes version.
```
kubectl version --short
```

Use the following command to check the `Container Runtime Interface` health.
```
docker exec -it kind-control-plane sh -c 'crictl info | jq ".lastCNILoadStatus,.config.cni,.cniconfig"'
```

By default CRI will look into `/opt/cni/bin/` directory to locate the CNI binary files and `/etc/cni/net.d` for the CNI configuration.

Use the following commands to check the content of these two folders.
```
docker exec -it kind-control-plane ls /opt/cni/bin/
docker exec -it kind-control-plane ls /etc/cni/net.d/
```

Use the following command to install the latest Calico CRDs.
```
kubectl apply -f https://projectcalico.docs.tigera.io/manifests/crds.yaml
```

Use the following command to install the `calicoctl` standalone application on your system.
> **Note:** This is a `x86_64` binary file. Binary file for other system architectures could be found [here](https://github.com/projectcalico/calicoctl/releases).
```
curl -OL  https://github.com/projectcalico/calicoctl/releases/download/v3.20.0/calicoctl
sudo chmod +x calicoctl
sudo mv calicoctl /usr/local/bin/
```

Use the following command to list all the participating nodes.
```
calicoctl get nodes
```

Use the following command to list the ippools.
```
calicoctl get ippools
```

Use the following command to determine the CIDR used at the time of bootstrapping the cluster.
```
kubectl cluster-info dump | egrep -i "\--cluster-cidr"
```

Use the following command to install the initial Calico ippool.
```
calicoctl apply -f - <<EOF
apiVersion: projectcalico.org/v3
kind: IPPool
metadata:
  name: pool1
spec:
  cidr: 10.244.0.0/16
  ipipMode: Never
  natOutgoing: true
  disabled: false
  nodeSelector: all()
EOF
```

Use the following command to check the Node status.
```
kubectl get node
```

Use the following command to connect to the control-plane terminal.
```
docker exec -it kind-control-plane /bin/bash
```

Use the following command to create a folder for certificates and browse into it.
```
mkdir ~/calicocerts/ && cd ~/calicocerts
```

Use the following command to create a certificate request file.
```
openssl req -newkey rsa:4096 \
           -keyout cni.key \
           -nodes \
           -out cni.csr \
           -subj "/CN=calico-cni"
```

Use the following command to sign the CSR file with the Kubernetes certificate authority key.
```
openssl x509 -req -in cni.csr \
                  -CA /etc/kubernetes/pki/ca.crt \
                  -CAkey /etc/kubernetes/pki/ca.key \
                  -CAcreateserial \
                  -out cni.crt \
                  -days 365
```

Use the following command to store the Kubernetes API server address in an environment variable.
```
APISERVER=$(kubectl config view -o jsonpath='{.clusters[0].cluster.server}')
```


```
kubectl config set-cluster kubernetes \
    --certificate-authority=/etc/kubernetes/pki/ca.crt \
    --embed-certs=true \
    --server=$APISERVER \
    --kubeconfig=cni.kubeconfig
```

```
kubectl config set-credentials calico-cni \
    --client-certificate=cni.crt \
    --client-key=cni.key \
    --embed-certs=true \
    --kubeconfig=cni.kubeconfig
```

```
kubectl config set-context default \
    --cluster=kubernetes \
    --user=calico-cni \
    --kubeconfig=cni.kubeconfig
```

```
kubectl config use-context default --kubeconfig=cni.kubeconfig
```

Create a configmap and store the kubeconfig file that will be used by the Calico CNI.
```
kubectl create configmap -n kube-system cni.kubeconfig --from-file=cni.kubeconfig
```

Create the necessary permissions for the CNI.
```
kubectl apply -f - <<EOF
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: calico-cni
rules:
  # The CNI plugin needs to get pods, nodes, and namespaces.
  - apiGroups: [""]
    resources:
      - pods
      - nodes
      - namespaces
    verbs:
      - get
  # The CNI plugin patches pods/status.
  - apiGroups: [""]
    resources:
      - pods/status
    verbs:
      - patch
 # These permissions are required for Calico CNI to perform IPAM allocations.
  - apiGroups: ["crd.projectcalico.org"]
    resources:
      - blockaffinities
      - ipamblocks
      - ipamhandles
    verbs:
      - get
      - list
      - create
      - update
      - delete
  - apiGroups: ["crd.projectcalico.org"]
    resources:
      - ipamconfigs
      - clusterinformations
      - ippools
    verbs:
      - get
      - list
EOF
```

Bind the roles that we created earlier to the cluster.
```
kubectl create clusterrolebinding calico-cni --clusterrole=calico-cni --user=calico-cni
```

Download the Calico CNI and IPAM binary files.
```
curl -L -o /opt/cni/bin/calico https://github.com/projectcalico/cni-plugin/releases/download/v3.14.0/calico-amd64
chmod 755 /opt/cni/bin/calico
curl -L -o /opt/cni/bin/calico-ipam https://github.com/projectcalico/cni-plugin/releases/download/v3.14.0/calico-ipam-amd64
chmod 755 /opt/cni/bin/calico-ipam
```

Copy the kubeconfig file in the cni configuration folder.
```
cp cni.kubeconfig /etc/cni/net.d/calico-kubeconfig
chmod 600 /etc/cni/net.d/calico-kubeconfig
```

Create the `conflist` file for kubelet to use Calico.
```
cat > /etc/cni/net.d/10-calico.conflist <<EOF
{
  "name": "k8s-pod-network",
  "cniVersion": "0.3.1",
  "plugins": [
    {
      "type": "calico",
      "log_level": "info",
      "datastore_type": "kubernetes",
      "mtu": 1500,
      "ipam": {
          "type": "calico-ipam"
      },
      "policy": {
          "type": "k8s"
      },
      "kubernetes": {
          "kubeconfig": "/etc/cni/net.d/calico-kubeconfig"
      }
    },
    {
      "type": "portmap",
      "snat": true,
      "capabilities": {"portMappings": true}
    }
  ]
}
EOF
```

Use the following command to check the status of your node once again.
```
kubectl get nodes
```

You should see a result similar to the following.
```
NAME                 STATUS     ROLES           AGE     VERSION
kind-control-plane   Ready      control-plane   2m57s   v1.24.0
kind-worker          NotReady   <none>          2m21s   v1.24.0
kind-worker2         NotReady   <none>          2m21s   v1.24.0
```

Use the following command to end the control-plane terminal session.
```
exit
```

Use the following command to copy the config file that we created in the previous steps.
```
docker exec -it kind-control-plane cat /etc/cni/net.d/calico-kubeconfig > calico-kubeconfig
```

Copy the configfile in the remaining nodes.
```
docker cp calico-kubeconfig kind-worker:/etc/cni/net.d/calico-kubeconfig
docker cp calico-kubeconfig kind-worker2:/etc/cni/net.d/calico-kubeconfig
```

Use the following command to invoke the shell script that automates the previous steps to speed-up the process.
```
docker exec -it kind-worker sh -c /root/cni-installer.sh
docker exec -it kind-worker2 sh -c /root/cni-installer.sh
```

Check the node status once more, all nodes should now be ready.
```
kubectl get nodes
```
